//
// Copyright 2017 GoDataDriven B.V.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//

divolte {
  global {
    server {
      // The host to which the server binds.
      // Set to a specific IP address to selectively listen on that interface.
      // If not present, a loopback-only address will be bound.
      host = 0.0.0.0
      // The bind host can be overridden using the DIVOLTE_HOST environment variable.
      host = ${?DIVOLTE_HOST}

      debug_requests = false
      // The debug option can be overridden.
      debug_requests = ${?DIVOLTE_DEBUG_REQUESTS}
    }

    hdfs {
      // If true, flushing to HDFS is enabled.
      enabled = false
      enabled = ${?DIVOLTE_HDFS_ENABLED}

      // Number of threads to use for flushing events to HDFS.
      // Each thread creates its own files on HDFS. Depending
      // on the flushing strategy, multiple concurrent files
      // could be kept open per thread.
      threads = 2

      // The maximum queue of mapped events to buffer before
      // starting to drop new ones. Note that when this buffer is full,
      // events are dropped and a warning is logged. No errors are reported
      // to the source of the events. A single buffer is shared between all
      // threads, and its size will be rounded up to the nearest power of 2.
      buffer_size = 1048576

      // Arbitrary HDFS client properties.
      // If absent, hdfs-site.xml from the classpath will be used.
      //client {}
    }
    
    gcs {
      // If true, flushing to Google Cloud Storage is enabled.
      enabled = false
      enabled = ${?DIVOLTE_GFS_ENABLED}


      // Number of threads to use for flushing events to Google Cloud Storage. Each
      // thread creates its own files on Google Cloud Storage. Depending on the
      // flushing strategy, multiple concurrent files could be kept open per thread.
      threads = 1

      // The maximum queue of mapped events to buffer before
      // starting to drop new ones. Note that when this buffer is full,
      // events are dropped and a warning is logged. No errors are reported
      // to the source of the events. A single buffer is shared between all
      // threads, and its size will be rounded up to the nearest power of 2.
      buffer_size = 1048576
    }
    
    kafka {
      // If true, flushing to Kafka is enabled.

      enabled = true
      enabled = ${?DIVOLTE_KAFKA_ENABLED}


      // Number of threads to use for flushing events to Kafka
      threads = 2

      // The maximum queue of mapped events to buffer before
      // starting to drop new ones. Note that when this buffer is full,
      // events are dropped and a warning is logged. No errors are reported
      // to the source of the events. A single buffer is shared between all
      // threads, and its size will be rounded up to the nearest power of 2.
      buffer_size = 1048576

      // All settings in here are used as-is to configure
      // the Kafka producer.
      // See: http://kafka.apache.org/082/documentation.html#newproducerconfigs
      producer = {
        bootstrap.servers = ["localhost:9092"]
        bootstrap.servers = ${?DIVOLTE_KAFKA_BROKER_LIST}
        client.id = divolte.collector
        client.id = ${?DIVOLTE_KAFKA_CLIENT_ID}

        acks = 1
        retries = 0
        compression.type = lz4
        max.in.flight.requests.per.connection = 1

        sasl.jaas.config = ""
        sasl.jaas.config = ${?KAFKA_SASL_JAAS_CONFIG}

        security.protocol = PLAINTEXT
        security.protocol = ${?KAFKA_SECURITY_PROTOCOL}
        sasl.mechanism = GSSAPI
        sasl.kerberos.service.name = kafka
      }
    }
  }
  
  sources {
    browser = {
      type = browser
    }
    // Here's the low-level JSON source we're adding.
    json = {
      type = json
      event_path = /json
    }
  }

  mappings {
    my_mapping = {
      schema_file = /etc/shop/divolte/ShopEventRecord.avsc
      mapping_script_file = /etc/shop/divolte/mapping.groovy
      sources = [browser]
      sinks = [kafka]
    }
    file_mapping = {
      schema_file = /etc/shop/divolte/ShopEventRecord.avsc
      mapping_script_file = /etc/shop/divolte/mapping.groovy
      sources = [json]
      sinks = [hdfs]
    }
  }

  sinks {
    kafka {
      type = kafka
      topic = divolte
      topic = ${?DIVOLTE_KAFKA_TOPIC}
    }

    hdfs = {
      type = hdfs
      file_strategy = {
        sync_file_after_records = 1000
        sync_file_after_records = ${?DIVOLTE_HDFS_SINK_SYNC_NR_OF_RECORDS}
        sync_file_after_duration = 30 minutes
        sync_file_after_duration = ${?DIVOLTE_HDFS_SINK_SYNC_DURATION}
        roll_every = 60 minutes
        roll_every = ${?DIVOLTE_HDFS_SINK_ROLL_DURATION}
        working_dir = /tmp/working
        working_dir = ${?DIVOLTE_HDFS_SINK_WORKING_DIR}
        publish_dir = /tmp/processed
        publish_dir = ${?DIVOLTE_HDFS_SINK_PUBLISH_DIR}
      }
    }
  }
}
